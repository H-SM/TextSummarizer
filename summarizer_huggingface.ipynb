{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text summarizer with Hugging Face models\n",
    "\n",
    "<img src=\"https://www.thesoftwarereport.com/wp-content/uploads/2023/09/Hugging-Face2.png\" width=\"500\"/>\n",
    "\n",
    "<!-- ![hugging face](https://www.thesoftwarereport.com/wp-content/uploads/2023/09/Hugging-Face2.png) -->\n",
    "Major Steps over covering the model \n",
    "- Setting up Hugging face Summarization Pipeline \n",
    "- Scrape blog posts (medium, adobe) for net using beautiful soup \n",
    "- chunk them into sentences and summarize over these chunks (using the model to output over in text)\n",
    "\n",
    "```bash\n",
    "why are we chunking here? \n",
    "Limit over characters to be passed over the model - less need of GPU \n",
    "```\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 1. Installing Tranformers and Importing Dependencies "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline # get our summarization model\n",
    "from bs4 import BeautifulSoup # scrap up blog post from the web \n",
    "import requests # make over http calls over to web (to scrap up a page)\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2. Load our summarization Pipeline  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No model was supplied, defaulted to google-t5/t5-small and revision d769bba (https://huggingface.co/google-t5/t5-small).\n",
      "Using a pipeline without specifying a model name and revision in production is not recommended.\n",
      "All PyTorch model weights were used when initializing TFT5ForConditionalGeneration.\n",
      "\n",
      "All the weights of TFT5ForConditionalGeneration were initialized from the PyTorch model.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFT5ForConditionalGeneration for predictions without further training.\n"
     ]
    }
   ],
   "source": [
    "summarizer = pipeline('summarization') # hold up our summarization pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3. making web-scrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "URL = \"https://medium.com/@deepeshrishu09/automatic-image-captioning-with-pytorch-cf576c98d319\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = requests.get(URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(r.text, 'html.parser')\n",
    "results = soup.find_all(['h1', 'p'])\n",
    "text = [result.text for result in results]\n",
    "ARTICLE = ' '.join(text)\n",
    "script_tag = soup.find('script', type='application/ld+json')\n",
    "\n",
    "if script_tag:\n",
    "    # Parse JSON data\n",
    "    json_data = json.loads(script_tag.string)\n",
    "    \n",
    "    # Extract articleBody\n",
    "    article_body = json_data.get('articleBody', '')\n",
    "\n",
    "    print(article_body)\n",
    "\n",
    "    ARTICLE = ARTICLE +\". \" + article_body\n",
    "else:\n",
    "    print(\"JSON-LD script tag not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sign up Sign in Sign up Sign in Automatic Image Captioning With PyTorch Deepesh Garg Follow -- Listen Share ‚ÄúIt‚Äôs going to be interesting to see how society deals with artificial intelligence, but it will definitely be cool.‚Äù - Colin Angle This is my first open source project . I was selected as a Participant for Open Source Contributions at Student Code-in . Actually, It was a two months programme where I was selected for contributions to a Computer Vision Project : Image Captioning . In this project, I design and train a CNN-RNN (Convolutional Neural Network ‚Äî Recurrent Neural Network) model for automatically generating image captions. In this case, LSTM (Long Short Term Memory), is used which is a special kind of RNN that includes a memory cell, in order to maintain the information for a longer period of time. The network is trained on the Microsoft Common Objects in COntext (MS COCO) dataset. The image captioning model is displayed below. Image Source Dataset Used ‚Äî MS COCO Dataset The COCO dataset is one of the largest, publicly available image datasets and it is meant to represent realistic scenes. What I mean by this is that COCO does not overly pre-process images, instead these images come in a variety of shapes with a variety of objects and environment/lighting conditions that closely represent what you might get if you compiled images from many different cameras around the world. To explore the dataset, you can check out the dataset website Captions COCO is a richly labeled dataset; it comes with class labels, labels for segments of an image, and a set of captions for a given image . Here is an example : Image Source ‚Äî Udacity Visualize the Dataset The Microsoft Common Objects in COntext (MS COCO) dataset is a large-scale dataset for scene understanding. The dataset is commonly used to train and benchmark object detection, segmentation, and captioning algorithms. The CNN-RNN Architecture Encoder CNN The encoder that I used was the pre-trained ResNet-50 architecture (with the final fully-connected layer removed) to extract features from a batch of pre-processed images. The output is then flattened to a vector, before being passed through a Linear layer to transform the feature vector to have the same size as the word embedding. Decoder RNN The job of the RNN is to decode the process vector and turn it into a sequence of words. Thus, this portion of the network is often called a decoder. In this case, LSTM (Long Short Term Memory), is used which is a special kind of RNN that includes a memory cell, in order to maintain the information for a longer period of time. Caption Pre-Processing The captions also need to be pre-processed and prepped for training. In this example, for generating captions, I aimed to create a model that predicts the next token of a sentence from previous tokens, So I turned the caption associated with any image into a list of tokenized words, before casting it to a PyTorch tensor that we can use to train the network. Tokenizing Captions First, we iterate through all of the training captions and create a dictionary that maps all unique words to a numerical index. So, every word we come across will have a corresponding integer value that can be found in this dictionary. The words in this dictionary are referred to as vocabulary. Output Output Conversion Of Word To Vectors The words first must be turned into a numerical representation so that a network can use normal loss functions and optimizers to calculate the difference between a predicted word and ground truth word (from a known, training caption) . So, we typically turn a sequence of words into a sequence of numerical values; a vector of numbers where each number maps to a specific word in our vocabulary. Training The Model We have two model components, i.e. encoder and decoder, we train them jointly by passing the output of the encoder, which is the latent space vector, to the decoder, which, in turn, is the recurrent neural network. No. Of Epochs = 1 Batch Size = 32 To figure out how well our model is doing, we can look at how the training loss and perplexity evolve during training ‚Äî and for the purposes of this project, we can amend the hyperparameters based on this information. However, this will not tell you if your model is overfitting to the training data, and, unfortunately, overfitting is a problem that is commonly encountered when training image captioning models. For this project, you need not worry about overfitting. This project does not have strict requirements regarding the performance of your model, and you just need to demonstrate that your model has learned something when you generate captions on the test data. Prediction Function The get_prediction function was used to loop over images in the test dataset and print model's predicted caption. Predicted Results More Predictions This is my complete open source project on GitHub . References 1. Show, Attend and Tell: Neural Image Caption Generation with Visual Attention( https://arxiv.org/pdf/1502.03044.pdf) 2. https://github.com/sauravraghuvanshi/Udacity-Computer-Vision-Nanodegree-Program/tree/master/project_2_image_captioning_project Happy Learning ! -- -- CSE Undergrad üéì || ML Enthusiast üíª || Coder üë®‚Äçüíª || Optimistic About New And Developing AI Technologies üíØ Help Status About Careers Blog Privacy Terms Text to speech Teams. \n"
     ]
    }
   ],
   "source": [
    "print(ARTICLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
